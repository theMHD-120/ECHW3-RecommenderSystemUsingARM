{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219cfa07",
   "metadata": {},
   "source": [
    "### in the name of Allah\n",
    "# Shopping Cart Analysis and Recommender System based on ARM (Association Rule Mining)\n",
    "------------------\n",
    "## TASK I: Data Preprocessing\n",
    "\n",
    "This task focuses on preparing the Instacart e-commerce dataset for Association Rule Mining analysis. The preprocessing pipeline ensures data quality and creates a manageable subset for efficient pattern mining. We address common data issues including missing values and irrelevant transactions, while focusing on multi-item baskets essential for meaningful association rule discovery. By sampling 20,000 users, we balance computational feasibility with sufficient data coverage for robust analysis.\n",
    "\n",
    "### **Step 1: Loading the Data**\n",
    "Load CSV files:\n",
    "- **pandas** for small files: `aisles.csv`, `departments.csv`, `products.csv`\n",
    "- **dask** for large files: `order_products__train.csv`, `orders.csv`, `order_products__prior.csv`\n",
    "\n",
    "### **Step 2: Data Cleaning Functions**\n",
    "Define functions:\n",
    "- **`remove_nulls(df)`**: Remove rows with missing values.\n",
    "- **`filter_single_item_orders(df)`**: Remove orders with only one product.\n",
    "- **`filter_by_order_ids(df, order_ids_set)`**: Filter by order IDs.\n",
    "\n",
    "### **Step 3: Preprocessing Execution**\n",
    "1. **Remove nulls** from all tables.\n",
    "2. **Remove single-item orders** from order products data.\n",
    "3. **Sample 20,000 users** randomly from orders.\n",
    "4. **Extract orders** for sampled users.\n",
    "5. **Filter order products** to include only sampled orders.\n",
    "\n",
    "### **Step 4: Saving Cleaned Data**\n",
    "Save cleaned data to `./processed_data/`:\n",
    "- **`aisles_cleaned.csv`**, **`products_cleaned.csv`**, **`departments_cleaned.csv`**\n",
    "- **`orders_sampled.csv`**: Orders of 20,000 sampled users.\n",
    "- **`order_products_train_sampled.csv`**, **`order_products_prior_sampled.csv`**\n",
    "- **`order_products_combined.csv`**: Combined data for basket analysis.\n",
    "\n",
    "This preprocessing ensures clean, multi-item basket data ready for Association Rule Mining in subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dcc1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed.\n",
      "Orders: 3,421,083 rows\n",
      "Order Products Prior: 32,434,489 rows\n",
      "Order Products Train: 1,384,617 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING DATA FROM CSV FILES\n",
    "# ============================================================================\n",
    "\n",
    "# Load small metadata files using pandas\n",
    "aisles = pd.read_csv('aisles.csv')\n",
    "products = pd.read_csv('products.csv')\n",
    "departments = pd.read_csv('departments.csv')\n",
    "\n",
    "# Load large transactional files using dask\n",
    "orders = dd.read_csv('orders.csv')\n",
    "order_products_train = dd.read_csv('order_products__train.csv')\n",
    "order_products_prior = dd.read_csv('order_products__prior.csv')\n",
    "\n",
    "print(\"Data loading completed.\")\n",
    "print(f\"Orders: {orders.shape[0].compute():,} rows\")\n",
    "print(f\"Order Products Prior: {order_products_prior.shape[0].compute():,} rows\")\n",
    "print(f\"Order Products Train: {order_products_train.shape[0].compute():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_nulls(df):\n",
    "    \"\"\"Remove rows with null values from dataframe.\"\"\"\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def filter_single_item_orders(df):\n",
    "    \"\"\"\n",
    "    Remove orders that contain only one product item.\n",
    "    For basket analysis, we need at least two items per order.\n",
    "    \"\"\"\n",
    "    # Count products per order using groupby and size\n",
    "    item_counts = df.groupby('order_id').size().reset_index()\n",
    "    item_counts = item_counts.rename(columns={0: 'item_count'})\n",
    "    \n",
    "    # Get orders with more than 1 item & filter original dataframe\n",
    "    multi_item_orders = item_counts[item_counts['item_count'] > 1]\n",
    "    return df[df['order_id'].isin(multi_item_orders['order_id'])]\n",
    "\n",
    "\n",
    "def filter_by_order_ids(df, order_ids_set):\n",
    "    \"\"\"\n",
    "    Filter dataframe based on a set of order_ids\n",
    "    >>> used for 2000 selected users.\n",
    "    \"\"\"\n",
    "    return df[df['order_id'].isin(order_ids_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e097baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TASK 1: DATA PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "1. Removing null values...\n",
      "2. Removing single-item orders...\n",
      "3. Sampling 20,000 random users...\n",
      "4. Filtering order products...\n",
      "\n",
      "Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: DATA PREPROCESSING EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 1: DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Remove null values\n",
    "print(\"\\n1. Removing null values...\")\n",
    "aisles_cleaned = remove_nulls(aisles)\n",
    "products_cleaned = remove_nulls(products)\n",
    "departments_cleaned = remove_nulls(departments)\n",
    "orders_cleaned = remove_nulls(orders).persist()\n",
    "order_products_train_cleaned = remove_nulls(order_products_train).persist()\n",
    "order_products_prior_cleaned = remove_nulls(order_products_prior).persist()\n",
    "\n",
    "# Step 2: Remove single-item orders\n",
    "print(\"2. Removing single-item orders...\")\n",
    "order_products_train_filtered = filter_single_item_orders(order_products_train_cleaned).persist()\n",
    "order_products_prior_filtered = filter_single_item_orders(order_products_prior_cleaned).persist()\n",
    "\n",
    "# Step 3: Sample 20,000 users\n",
    "print(\"3. Sampling 20,000 random users...\")\n",
    "unique_users_count = orders_cleaned['user_id'].nunique().compute()\n",
    "frac_value = min(20000 / unique_users_count, 1.0)\n",
    "\n",
    "sampled_users = orders_cleaned['user_id'].drop_duplicates().sample(\n",
    "    frac=frac_value, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Get orders for sampled users\n",
    "sampled_users_list = sampled_users.compute().tolist()\n",
    "orders_sampled = orders_cleaned[orders_cleaned['user_id'].isin(sampled_users_list)]\n",
    "\n",
    "# Get order IDs\n",
    "sampled_order_ids = orders_sampled['order_id'].compute().tolist()\n",
    "order_ids_set = set(sampled_order_ids)\n",
    "\n",
    "# Step 5: Filter order products\n",
    "print(\"4. Filtering order products...\")\n",
    "order_products_train_sampled = order_products_train_filtered.map_partitions(\n",
    "    filter_by_order_ids, \n",
    "    order_ids_set,\n",
    "    meta=order_products_train_filtered._meta\n",
    ").persist()\n",
    "\n",
    "order_products_prior_sampled = order_products_prior_filtered.map_partitions(\n",
    "    filter_by_order_ids,\n",
    "    order_ids_set,\n",
    "    meta=order_products_prior_filtered._meta\n",
    ").persist()\n",
    "\n",
    "print(\"\\nPreprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66e80146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to ./processed_data/\n",
      "\n",
      "==================================================\n",
      "TASK 1 SUMMARY\n",
      "==================================================\n",
      "Sampled users: 20,000\n",
      "Sampled orders: 312,778\n",
      "Total products for basket analysis: 3,083,695\n",
      "\n",
      "✓ Task 1 completed - Data ready for basket analysis (Task 2)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVING PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "output_folder = './processed_data'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "print(f\"\\nSaving data to {output_folder}/\")\n",
    "\n",
    "# Save metadata files\n",
    "aisles_cleaned.to_csv(f'{output_folder}/aisles_cleaned.csv', index=False)\n",
    "products_cleaned.to_csv(f'{output_folder}/products_cleaned.csv', index=False)\n",
    "departments_cleaned.to_csv(f'{output_folder}/departments_cleaned.csv', index=False)\n",
    "\n",
    "# Save transactional data (need to compute dask dataframes first)\n",
    "orders_sampled_computed = orders_sampled.compute()\n",
    "orders_sampled_computed.to_csv(f'{output_folder}/orders_sampled.csv', index=False)\n",
    "\n",
    "order_products_train_sampled_computed = order_products_train_sampled.compute()\n",
    "order_products_train_sampled_computed.to_csv(\n",
    "    f'{output_folder}/order_products_train_sampled.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "order_products_prior_sampled_computed = order_products_prior_sampled.compute()\n",
    "order_products_prior_sampled_computed.to_csv(\n",
    "    f'{output_folder}/order_products_prior_sampled.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Save combined data for basket analysis\n",
    "order_products_combined = dd.concat([order_products_train_sampled, order_products_prior_sampled])\n",
    "order_products_combined_computed = order_products_combined.compute()\n",
    "order_products_combined_computed.to_csv(\n",
    "    f'{output_folder}/order_products_combined.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 1 SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Sampled users: {len(sampled_users_list):,}\")\n",
    "print(f\"Sampled orders: {orders_sampled_computed.shape[0]:,}\")\n",
    "print(f\"Total products for basket analysis: {order_products_combined_computed.shape[0]:,}\")\n",
    "print(\"\\n✓ Task 1 completed - Data ready for basket analysis (Task 2)\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453f010",
   "metadata": {},
   "source": [
    "## TASK II: Base Demand Calculation and Initial Strategy\n",
    "\n",
    "In this task, we calculate the base demand for each product, define an initial pricing, advertising strategy and Demand and Profit functions. This includes:\n",
    "\n",
    "1. **Calculate base demand**: The base demand for each product is calculated as the total quantity divided by the number of active months. If no active months are available, total quantity is used.\n",
    "2. **Assign cost**: Based on the median price of the products, we assign a cost of 5 or 10 units.\n",
    "3. **Define initial strategy**: The initial price is set to the average price, and advertising budget is initialized.\n",
    "4. **Save results**: The results are saved for the next simulation stage.\n",
    "5. **Demand Function (demand_func)**: Calculates the demand based on price, average price of other sellers, advertising budget, and social influence.\n",
    "6. **Profit Function (profit_func)**: Calculates the profit for each seller based on the demand, price, and advertising budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef26bd0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processed_data/cleaned_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the cleaned data and extract only the necessary columns related to 'order_id' and 'product_id'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cleaned_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cleaned_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m order_products_train_cleaned = cleaned_data[[\u001b[33m'\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m]]  \u001b[38;5;66;03m# Only the relevant columns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Group by `order_id` to create a list of products for each order\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './processed_data/cleaned_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Load the cleaned data and extract only the necessary columns related to 'order_id' and 'product_id'\n",
    "cleaned_data = pd.read_csv(f'{output_folder}/cleaned_data.csv')\n",
    "order_products_train_cleaned = cleaned_data[['order_id', 'product_id']]  # Only the relevant columns\n",
    "\n",
    "# Group by `order_id` to create a list of products for each order\n",
    "grouped_orders = order_products_train_cleaned.groupby('order_id')['product_id'].apply(list).reset_index(name='product_list')\n",
    "\n",
    "# Create one-hot encoding for the products in each order using TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit_transform(grouped_orders['product_list'])\n",
    "one_hot_encoded_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the first few rows of and save the One-Hot Encoded table to verify the transformation\n",
    "print(one_hot_encoded_df.head())\n",
    "one_hot_encoded_df.to_csv(f'{output_folder}/one_hot_encoded_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
