{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219cfa07",
   "metadata": {},
   "source": [
    "### in the name of Allah\n",
    "# Shopping Cart Analysis and Recommender System based on ARM (Association Rule Mining)\n",
    "------------------\n",
    "## TASK I: Data Preprocessing\n",
    "\n",
    "This task focuses on preparing the Instacart e-commerce dataset for Association Rule Mining analysis. The preprocessing pipeline ensures data quality and creates a manageable subset for efficient pattern mining. We address common data issues including missing values and irrelevant transactions, while focusing on multi-item baskets essential for meaningful association rule discovery. By sampling 20,000 users, we balance computational feasibility with sufficient data coverage for robust analysis.\n",
    "\n",
    "### **Step 1: Loading the Data**\n",
    "Load CSV files:\n",
    "- **pandas** for small files: `aisles.csv`, `departments.csv`, `products.csv`\n",
    "- **dask** for large files: `order_products__train.csv`, `orders.csv`, `order_products__prior.csv`\n",
    "\n",
    "### **Step 2: Data Cleaning Functions**\n",
    "Define functions:\n",
    "- **`remove_nulls(df)`**: Remove rows with missing values.\n",
    "- **`filter_single_item_orders(df)`**: Remove orders with only one product.\n",
    "- **`filter_by_order_ids(df, order_ids_set)`**: Filter by order IDs.\n",
    "\n",
    "### **Step 3: Preprocessing Execution**\n",
    "1. **Remove nulls** from all tables.\n",
    "2. **Remove single-item orders** from order products data.\n",
    "3. **Sample 20,000 users** randomly from orders.\n",
    "4. **Extract orders** for sampled users.\n",
    "5. **Filter order products** to include only sampled orders.\n",
    "\n",
    "### **Step 4: Saving Cleaned Data**\n",
    "Save cleaned data to `./processed_data/`:\n",
    "- **`aisles_cleaned.csv`**, **`products_cleaned.csv`**, **`departments_cleaned.csv`**\n",
    "- **`orders_sampled.csv`**: Orders of 20,000 sampled users.\n",
    "- **`order_products_train_sampled.csv`**, **`order_products_prior_sampled.csv`**\n",
    "- **`order_products_combined.csv`**: Combined data for basket analysis.\n",
    "\n",
    "This preprocessing ensures clean, multi-item basket data ready for Association Rule Mining in subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc1d5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrequent_patterns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apriori\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# LOADING DATA FROM CSV FILES\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load small metadata files using pandas\u001b[39;00m\n\u001b[32m     14\u001b[39m aisles = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33maisles.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\mlxtend\\preprocessing\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Sebastian Raschka 2014-2026\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# mlxtend Machine Learning Library Extensions\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Sebastian Raschka <sebastianraschka.com>\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# License: BSD 3 clause\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcopy_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CopyTransformer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense_transformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseTransformer\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmean_centering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MeanCenterer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\mlxtend\\preprocessing\\copy_transformer.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCopyTransformer\u001b[39;00m(BaseEstimator):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transformer that returns a copy of the input array\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m    For usage examples, please see\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    https://rasbt.github.io/mlxtend/user_guide/preprocessing/CopyTransformer/\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\__init__.py:70\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build, _distributor_init  \u001b[38;5;66;03m# noqa: E402 F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     73\u001b[39m _submodules = [\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_pandas_na, is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, resample, shuffle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:24\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     DataConversionWarning,\n\u001b[32m     21\u001b[39m     NotFittedError,\n\u001b[32m     22\u001b[39m     PositiveSpectrumWarning,\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     _asarray_with_order,\n\u001b[32m     26\u001b[39m     _convert_to_numpy,\n\u001b[32m     27\u001b[39m     _is_numpy_namespace,\n\u001b[32m     28\u001b[39m     _max_precision_float_dtype,\n\u001b[32m     29\u001b[39m     get_namespace,\n\u001b[32m     30\u001b[39m     get_namespace_and_device,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_pandas_df, is_pandas_df_or_series\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_isfinite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_df_or_series\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\sklearn\\utils\\fixes.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Import unused here but needs to stay until end of deprecation periode\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_mstats_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _find_repeats, theilslopes, siegelslopes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\distributions.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distn_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_continuous_distns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_levy_stable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m levy_stable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\_discrete_distns.py:1480\u001b[39m\n\u001b[32m   1476\u001b[39m         g2 -= \u001b[32m3\u001b[39m\n\u001b[32m   1477\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m mu1, mu2, g1, g2\n\u001b[32m-> \u001b[39m\u001b[32m1480\u001b[39m zipfian = \u001b[43mzipfian_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mzipfian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mA Zipfian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdlaplace_gen\u001b[39;00m(rv_discrete):\n\u001b[32m   1484\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"A  Laplacian discrete random variable.\u001b[39;00m\n\u001b[32m   1485\u001b[39m \n\u001b[32m   1486\u001b[39m \u001b[33;03m    %(before_notes)s\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1503\u001b[39m \n\u001b[32m   1504\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:3348\u001b[39m, in \u001b[36mrv_discrete.__init__\u001b[39m\u001b[34m(self, a, b, name, badvalue, moment_tol, values, inc, longname, shapes, seed)\u001b[39m\n\u001b[32m   3343\u001b[39m \u001b[38;5;28mself\u001b[39m._construct_argparser(meths_to_inspect=[\u001b[38;5;28mself\u001b[39m._pmf, \u001b[38;5;28mself\u001b[39m._cdf],\n\u001b[32m   3344\u001b[39m                           locscale_in=\u001b[33m'\u001b[39m\u001b[33mloc=0\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   3345\u001b[39m                           \u001b[38;5;66;03m# scale=1 for discrete RVs\u001b[39;00m\n\u001b[32m   3346\u001b[39m                           locscale_out=\u001b[33m'\u001b[39m\u001b[33mloc, 1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3347\u001b[39m \u001b[38;5;28mself\u001b[39m._attach_methods()\n\u001b[32m-> \u001b[39m\u001b[32m3348\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_docstrings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:3401\u001b[39m, in \u001b[36mrv_discrete._construct_docstrings\u001b[39m\u001b[34m(self, name, longname)\u001b[39m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3400\u001b[39m     dct = \u001b[38;5;28mdict\u001b[39m(distdiscrete)\n\u001b[32m-> \u001b[39m\u001b[32m3401\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocdict_discrete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3403\u001b[39m \u001b[38;5;66;03m# discrete RV do not have the scale parameter, remove it\u001b[39;00m\n\u001b[32m   3404\u001b[39m \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m = \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m.replace(\n\u001b[32m   3405\u001b[39m     \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    scale : array_like, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3406\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moptional\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m        scale parameter (default=1)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:863\u001b[39m, in \u001b[36mrv_generic._construct_doc\u001b[39m\u001b[34m(self, docdict, shapes_vals)\u001b[39m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m = \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m.replace(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%(shapes)s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m = \u001b[43mdoccer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdocformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__doc__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempdict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to construct docstring for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    866\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdistribution \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\scipy\\_lib\\doccer.py:78\u001b[39m, in \u001b[36mdocformat\u001b[39m\u001b[34m(docstring, docdict)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, dstr \u001b[38;5;129;01min\u001b[39;00m docdict.items():\n\u001b[32m     77\u001b[39m     lines = dstr.expandtabs().splitlines()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     indented[name] = \u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m docstring % indented\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING DATA FROM CSV FILES\n",
    "# ============================================================================\n",
    "\n",
    "# Load small metadata files using pandas\n",
    "aisles = pd.read_csv('aisles.csv')\n",
    "products = pd.read_csv('products.csv')\n",
    "departments = pd.read_csv('departments.csv')\n",
    "\n",
    "# Load large transactional files using dask\n",
    "orders = dd.read_csv('orders.csv')\n",
    "order_products_train = dd.read_csv('order_products__train.csv')\n",
    "order_products_prior = dd.read_csv('order_products__prior.csv')\n",
    "\n",
    "print(\"Data loading completed.\")\n",
    "print(f\"Orders: {orders.shape[0].compute():,} rows\")\n",
    "print(f\"Order Products Prior: {order_products_prior.shape[0].compute():,} rows\")\n",
    "print(f\"Order Products Train: {order_products_train.shape[0].compute():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def remove_nulls(df):\n",
    "    \"\"\"Remove rows with null values from dataframe.\"\"\"\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def filter_single_item_orders(df):\n",
    "    \"\"\"\n",
    "    Remove orders that contain only one product item.\n",
    "    For basket analysis, we need at least two items per order.\n",
    "    \"\"\"\n",
    "    # Count products per order using groupby and size\n",
    "    item_counts = df.groupby('order_id').size().reset_index()\n",
    "    item_counts = item_counts.rename(columns={0: 'item_count'})\n",
    "    \n",
    "    # Get orders with more than 1 item & filter original dataframe\n",
    "    multi_item_orders = item_counts[item_counts['item_count'] > 1]\n",
    "    return df[df['order_id'].isin(multi_item_orders['order_id'])]\n",
    "\n",
    "\n",
    "def filter_by_order_ids(df, order_ids_set):\n",
    "    \"\"\"\n",
    "    Filter dataframe based on a set of order_ids\n",
    "    >>> used for 2000 selected users.\n",
    "    \"\"\"\n",
    "    return df[df['order_id'].isin(order_ids_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TASK 1: DATA PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "1. Removing null values...\n",
      "2. Removing single-item orders...\n",
      "3. Sampling 20,000 random users...\n",
      "4. Filtering order products...\n",
      "\n",
      "Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: DATA PREPROCESSING EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 1: DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Remove null values\n",
    "print(\"\\n1. Removing null values...\")\n",
    "aisles_cleaned = remove_nulls(aisles)\n",
    "products_cleaned = remove_nulls(products)\n",
    "departments_cleaned = remove_nulls(departments)\n",
    "orders_cleaned = remove_nulls(orders).persist()\n",
    "order_products_train_cleaned = remove_nulls(order_products_train).persist()\n",
    "order_products_prior_cleaned = remove_nulls(order_products_prior).persist()\n",
    "\n",
    "# Step 2: Remove single-item orders\n",
    "print(\"2. Removing single-item orders...\")\n",
    "order_products_train_filtered = filter_single_item_orders(order_products_train_cleaned).persist()\n",
    "order_products_prior_filtered = filter_single_item_orders(order_products_prior_cleaned).persist()\n",
    "\n",
    "# Step 3: Sample 20,000 users\n",
    "print(\"3. Sampling 20,000 random users...\")\n",
    "unique_users_count = orders_cleaned['user_id'].nunique().compute()\n",
    "frac_value = min(20000 / unique_users_count, 1.0)\n",
    "\n",
    "sampled_users = orders_cleaned['user_id'].drop_duplicates().sample(\n",
    "    frac=frac_value, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Get orders for sampled users\n",
    "sampled_users_list = sampled_users.compute().tolist()\n",
    "orders_sampled = orders_cleaned[orders_cleaned['user_id'].isin(sampled_users_list)]\n",
    "\n",
    "# Get order IDs\n",
    "sampled_order_ids = orders_sampled['order_id'].compute().tolist()\n",
    "order_ids_set = set(sampled_order_ids)\n",
    "\n",
    "# Step 5: Filter order products\n",
    "print(\"4. Filtering order products...\")\n",
    "order_products_train_sampled = order_products_train_filtered.map_partitions(\n",
    "    filter_by_order_ids, \n",
    "    order_ids_set,\n",
    "    meta=order_products_train_filtered._meta\n",
    ").persist()\n",
    "\n",
    "order_products_prior_sampled = order_products_prior_filtered.map_partitions(\n",
    "    filter_by_order_ids,\n",
    "    order_ids_set,\n",
    "    meta=order_products_prior_filtered._meta\n",
    ").persist()\n",
    "\n",
    "print(\"\\nPreprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e80146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to ./processed_data/\n",
      "\n",
      "==================================================\n",
      "TASK 1 SUMMARY\n",
      "==================================================\n",
      "Sampled users: 20,000\n",
      "Sampled orders: 312,778\n",
      "Total products for basket analysis: 3,083,695\n",
      "\n",
      "✓ Task 1 completed - Data ready for basket analysis (Task 2)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVING PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "output_folder = './processed_data'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "print(f\"\\nSaving data to {output_folder}/\")\n",
    "\n",
    "# Save metadata files\n",
    "aisles_cleaned.to_csv(f'{output_folder}/aisles_cleaned.csv', index=False)\n",
    "products_cleaned.to_csv(f'{output_folder}/products_cleaned.csv', index=False)\n",
    "departments_cleaned.to_csv(f'{output_folder}/departments_cleaned.csv', index=False)\n",
    "\n",
    "# Save transactional data (need to compute dask dataframes first)\n",
    "orders_sampled_computed = orders_sampled.compute()\n",
    "orders_sampled_computed.to_csv(f'{output_folder}/orders_sampled.csv', index=False)\n",
    "\n",
    "order_products_train_sampled_computed = order_products_train_sampled.compute()\n",
    "order_products_train_sampled_computed.to_csv(\n",
    "    f'{output_folder}/order_products_train_sampled.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "order_products_prior_sampled_computed = order_products_prior_sampled.compute()\n",
    "order_products_prior_sampled_computed.to_csv(\n",
    "    f'{output_folder}/order_products_prior_sampled.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Save combined data for basket analysis\n",
    "order_products_combined = dd.concat([order_products_train_sampled, order_products_prior_sampled])\n",
    "order_products_combined_computed = order_products_combined.compute()\n",
    "order_products_combined_computed.to_csv(\n",
    "    f'{output_folder}/order_products_combined.csv', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 1 SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Sampled users: {len(sampled_users_list):,}\")\n",
    "print(f\"Sampled orders: {orders_sampled_computed.shape[0]:,}\")\n",
    "print(f\"Total products for basket analysis: {order_products_combined_computed.shape[0]:,}\")\n",
    "print(\"\\n✓ Task 1 completed - Data ready for basket analysis (Task 2)\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453f010",
   "metadata": {},
   "source": [
    "## TASK II: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26bd0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processed_data/cleaned_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransactionEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the cleaned data and extract only the necessary columns related to 'order_id' and 'product_id'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cleaned_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cleaned_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m order_products_train_cleaned = cleaned_data[[\u001b[33m'\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m]]  \u001b[38;5;66;03m# Only the relevant columns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Group by `order_id` to create a list of products for each order\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ECHW\\HW3\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './processed_data/cleaned_data.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOADING PROCESSED DATA (FROM TASK1)\n",
    "# ============================================================================\n",
    "\n",
    "# Load order products data\n",
    "order_products = pd.read_csv('./processed_data/order_products_combined.csv')\n",
    "\n",
    "# Load products for mapping IDs to names\n",
    "products = pd.read_csv('./processed_data/products_cleaned.csv')\n",
    "\n",
    "print(f\"Order products: {order_products.shape[0]:,} rows\")\n",
    "print(f\"Unique orders: {order_products['order_id'].nunique():,}\")\n",
    "print(f\"Unique products: {order_products['product_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATING SHOPPING BASKETS\n",
    "# ============================================================================\n",
    "\n",
    "# Group by order_id to create product lists\n",
    "baskets = order_products.groupby('order_id')['product_id'].apply(list).reset_index(name='products')\n",
    "\n",
    "print(f\"Total baskets created: {len(baskets):,}\")\n",
    "print(f\"Avg items per basket: {baskets['products'].apply(len).mean():.2f}\")\n",
    "\n",
    "# Prepare transactions for one-hot encoding\n",
    "transactions = baskets['products'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc595514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATING ONE-HOT ENCODED MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize and fit transaction encoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# Create binary basket dataframe\n",
    "basket_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"Binary matrix shape: {basket_encoded.shape}\")\n",
    "print(f\"Products (columns): {basket_encoded.shape[1]}\")\n",
    "\n",
    "# Save for future use\n",
    "basket_encoded.to_csv('./processed_data/basket_encoded.csv', index=False)\n",
    "print(\"Saved: basket_encoded.csv\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 2 SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total baskets created: {len(baskets):,}\")\n",
    "print(f\"Unique products in baskets: {basket_encoded.shape[1]}\")\n",
    "print(f\"Binary matrix shape: {basket_encoded.shape}\")\n",
    "print(\"\\n✓ Task 2 completed - Basket data ready for Apriori algorithm\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa50b1e",
   "metadata": {},
   "source": [
    "## TASK III: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Apriori with different min_support values...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'apriori' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m results = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m min_sup \u001b[38;5;129;01min\u001b[39;00m min_support_values:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     frequent_itemsets = \u001b[43mapriori\u001b[49m(\n\u001b[32m     12\u001b[39m         basket_encoded, \n\u001b[32m     13\u001b[39m         min_support=min_sup,\n\u001b[32m     14\u001b[39m         use_colnames=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m         max_len=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m     results.append({\n\u001b[32m     19\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmin_support\u001b[39m\u001b[33m'\u001b[39m: min_sup,\n\u001b[32m     20\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_itemsets\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(frequent_itemsets),\n\u001b[32m     21\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmax_itemset_size\u001b[39m\u001b[33m'\u001b[39m: frequent_itemsets[\u001b[33m'\u001b[39m\u001b[33mitemsets\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28mlen\u001b[39m).max() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequent_itemsets) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     22\u001b[39m     })\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin_support=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_sup\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(frequent_itemsets)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m itemsets\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'apriori' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUNNING APRIORI WITH DIFFERENT < MIN SUPPORT > VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# Test different min_support values\n",
    "min_support_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]\n",
    "results = []\n",
    "\n",
    "for min_sup in min_support_values:\n",
    "    frequent_itemsets = apriori(\n",
    "        basket_encoded, \n",
    "        min_support=min_sup,\n",
    "        use_colnames=True,\n",
    "        max_len=None\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'min_support': min_sup,\n",
    "        'total_itemsets': len(frequent_itemsets),\n",
    "        'max_itemset_size': frequent_itemsets['itemsets'].apply(len).max() if len(frequent_itemsets) > 0 else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"min_support={min_sup:.2f}: {len(frequent_itemsets):,} itemsets\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResults summary:\")\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44642b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATING COMPARISON PLOTS\n",
    "# ============================================================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Total itemsets vs min_support\n",
    "ax1.plot(results_df['min_support'], results_df['total_itemsets'], 'b-o')\n",
    "ax1.set_xlabel('Minimum Support')\n",
    "ax1.set_ylabel('Total Frequent Itemsets')\n",
    "ax1.set_title('Itemsets vs Minimum Support')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Max itemset size vs min_support\n",
    "ax2.plot(results_df['min_support'], results_df['max_itemset_size'], 'r-o')\n",
    "ax2.set_xlabel('Minimum Support')\n",
    "ax2.set_ylabel('Maximum Itemset Size')\n",
    "ax2.set_title('Itemset Size vs Minimum Support')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./processed_data/apriori_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: apriori_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85edc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUNNING APRIORI WITH MIN_SUPPORT = 0.05\n",
    "# ============================================================================\n",
    "\n",
    "# Run Apriori with specified min_support\n",
    "frequent_itemsets_005 = apriori(\n",
    "    basket_encoded, \n",
    "    min_support=0.05,\n",
    "    use_colnames=True,\n",
    "    max_len=None\n",
    ")\n",
    "\n",
    "# Add itemset length\n",
    "frequent_itemsets_005['length'] = frequent_itemsets_005['itemsets'].apply(len)\n",
    "print(f\"Found {len(frequent_itemsets_005):,} frequent itemsets with min_support=0.05\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nItemset size distribution:\")\n",
    "size_dist = frequent_itemsets_005['length'].value_counts().sort_index()\n",
    "for size, count in size_dist.items():\n",
    "    print(f\"  Size {size}: {count:,} itemsets\")\n",
    "\n",
    "# Show top 10 itemsets\n",
    "print(\"\\nTop 10 frequent itemsets by support:\")\n",
    "top_10 = frequent_itemsets_005.nlargest(10, 'support')\n",
    "\n",
    "# Create product name mapping\n",
    "product_names = products.set_index('product_id')['product_name'].to_dict()\n",
    "for idx, row in top_10.iterrows():\n",
    "    items = list(row['itemsets'])\n",
    "    item_names = [product_names.get(item, f\"Product_{item}\") for item in items]\n",
    "    print(f\"  Support: {row['support']:.4f} - {', '.join(item_names[:2])}{'...' if len(item_names) > 2 else ''}\")\n",
    "\n",
    "# Save results\n",
    "frequent_itemsets_005.to_csv('./processed_data/frequent_itemsets_005.csv', index=False)\n",
    "print(\"\\nSaved: frequent_itemsets_005.csv\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 3 SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Frequent itemsets with min_support=0.05: {len(frequent_itemsets_005):,}\")\n",
    "print(f\"Itemset size distribution: {dict(size_dist)}\")\n",
    "print(\"\\n✓ Task 3 completed - Frequent itemsets generated for association rules\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcee35",
   "metadata": {},
   "source": [
    "## TASK IV: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOADING FREQUENT ITEMSETS FROM TASK 3\n",
    "# ============================================================================\n",
    "\n",
    "# Load frequent itemsets from Task 3\n",
    "frequent_itemsets = pd.read_csv('./processed_data/frequent_itemsets_005.csv')\n",
    "\n",
    "# Convert itemsets column from string to frozenset\n",
    "frequent_itemsets['itemsets'] = frequent_itemsets['itemsets'].apply(\n",
    "    lambda x: frozenset(ast.literal_eval(x)) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(frequent_itemsets):,} frequent itemsets\")\n",
    "print(f\"Itemset size range: {frequent_itemsets['length'].min()} to {frequent_itemsets['length'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACTING & ANALYZING ASSOCIATION RULES\n",
    "# ============================================================================\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(\n",
    "    frequent_itemsets, \n",
    "    metric=\"confidence\", \n",
    "    min_threshold=0.1\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(rules):,} association rules\")\n",
    "print(f\"\\nFirst 3 rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(3))\n",
    "\n",
    "# Analyzing rules metrics\n",
    "print(\"Analysis of rule metrics:\")\n",
    "print(f\"Support range: {rules['support'].min():.4f} to {rules['support'].max():.4f}\")\n",
    "print(f\"Confidence range: {rules['confidence'].min():.4f} to {rules['confidence'].max():.4f}\")\n",
    "print(f\"Lift range: {rules['lift'].min():.4f} to {rules['lift'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nAverage confidence: {rules['confidence'].mean():.4f}\")\n",
    "print(f\"Average lift: {rules['lift'].mean():.4f}\")\n",
    "print(f\"Rules with lift > 1: {(rules['lift'] > 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINDING TOP 3 RULES BY LIFT\n",
    "# ============================================================================\n",
    "\n",
    "# Get top 3 rules by lift\n",
    "top_3_lift = rules.nlargest(3, 'lift')\n",
    "\n",
    "print(\"Top 3 association rules by lift:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load product names for better interpretation\n",
    "products = pd.read_csv('./processed_data/products_cleaned.csv')\n",
    "product_names = products.set_index('product_id')['product_name'].to_dict()\n",
    "\n",
    "for idx, row in top_3_lift.iterrows():\n",
    "    antecedents = list(row['antecedents'])\n",
    "    consequents = list(row['consequents'])\n",
    "    \n",
    "    ant_names = [product_names.get(item, f\"Product_{item}\") for item in antecedents]\n",
    "    cons_names = [product_names.get(item, f\"Product_{item}\") for item in consequents]\n",
    "    \n",
    "    print(f\"\\nRule {idx+1}:\")\n",
    "    print(f\"  IF {', '.join(ant_names)}\")\n",
    "    print(f\"  THEN {', '.join(cons_names)}\")\n",
    "    print(f\"  Support: {row['support']:.4f}\")\n",
    "    print(f\"  Confidence: {row['confidence']:.4f}\")\n",
    "    print(f\"  Lift: {row['lift']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVING RULES AND FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "# Save all rules to CSV\n",
    "rules.to_csv('./processed_data/association_rules.csv', index=False)\n",
    "print(\"\\nSaved: association_rules.csv\")\n",
    "\n",
    "# Save top rules separately\n",
    "top_3_lift.to_csv('./processed_data/top_3_rules_by_lift.csv', index=False)\n",
    "print(\"Saved: top_3_rules_by_lift.csv\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TASK 4 SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total association rules generated: {len(rules):,}\")\n",
    "print(f\"Rules with lift > 1: {(rules['lift'] > 1).sum():,}\")\n",
    "print(f\"Top lift value: {top_3_lift['lift'].max():.4f}\")\n",
    "print(\"\\n✓ Task 4 completed - Association rules extracted and analyzed\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
